version: '3.9'

services:
  rag_server:
    build:
      context: ./RAG
      dockerfile: Dockerfile.rag
    ports:
      - "8000:8000"
    volumes:
      - ./RAG:/app
    networks:
      - chatbot_network

  rasa_server:
    build:
      context: ./RASA
      dockerfile: Dockerfile.rasa
    ports:
      - "5005:5005"
    volumes:
      - ./RASA:/app
    depends_on:
      - rasa_actions
    command: rasa run --enable-api --cors "*" --debug
    networks:
      - chatbot_network

  rasa_actions:
    build:
      context: ./RASA
      dockerfile: Dockerfile.actions
    ports:
      - "5055:5055"
    volumes:
      - ./RASA/actions:/app/actions
    networks:
      - chatbot_network

  client:
    build: 
      context: ./client
      dockerfile: Dockerfile.client
    ports:
      - "8080:80"
    networks:
      - chatbot_network

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"  # Ollama API port
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    command: serve
    networks:
      - chatbot_network

  axolotl:
    image: winglian/axolotl:main-py3.10-cu118-2.0.1
    container_name: axolotl-container
    volumes:
      - ./RAG/axolotl_configs:/app/configs
    command: python -m axolotl.cli /app/configs/config/llama3_finetune.yml
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    networks:
      - chatbot_network

volumes:
  ollama_data:

networks:
  chatbot_network:
